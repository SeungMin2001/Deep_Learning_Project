import os
os.environ["USE_TF"] = "0"
os.environ["USE_TORCH"] = "1"

# Compatibility patch for scipy.linalg.triu issue
import numpy as np
import scipy.linalg
if not hasattr(scipy.linalg, 'triu'):
    scipy.linalg.triu = np.triu

# Compatibility patch for huggingface_hub
try:
    from huggingface_hub import cached_download
except ImportError:
    from huggingface_hub import hf_hub_download as cached_download
    import huggingface_hub
    huggingface_hub.cached_download = cached_download

from tqdm import tqdm
import itertools
from umap import UMAP
from hdbscan import HDBSCAN
from bertopic import BERTopic
from gensim.corpora import Dictionary
from gensim.models.coherencemodel import CoherenceModel
import os
import json
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib.patches import Patch
from bertopic import BERTopic
import umap.umap_ as umap
import torch
import random
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm, tqdm_notebook
from torch.optim import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup
from kobert_transformers import get_kobert_model
import nltk
from itertools import product
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk import pos_tag
from sklearn.preprocessing import normalize
import pandas as pd
# Java ë¶ˆí•„ìš”í•œ í•œêµ­ì–´ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
try:
    from kiwipiepy import Kiwi
    USE_KIWI = True
except ImportError:
    try:
        from konlpy.tag import Okt
        USE_KIWI = False
    except ImportError:
        USE_KIWI = None
from transformers import BertModel, BertTokenizer
from transformers import AutoModel
from bertopic import BERTopic
from bertopic.vectorizers import ClassTfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
try:
    from ugtm import eGTM
except ImportError:
    eGTM = None
    print("Warning: ugtm not available due to compatibility issues")
import umap
import hdbscan
import re

class Step4:
    def ber(self):
        nltk.download('punkt')
        nltk.download('wordnet')
        nltk.download('stopwords')
        nltk.download('averaged_perceptron_tagger')

        file_path = './extract_end.csv'
        patent = pd.read_csv(file_path)

        summ = patent['astrtCont']+patent['ë°œëª…ëª…ì¹­']
        #summ=patent['ì²­êµ¬í•­']
        #print(summ)
        # í‘œì œì–´ ì¶”ì¶œê¸° ì„¤ì •
        lemmatizer = WordNetLemmatizer()


        def wordnet_pos_tags_kor(treebank_tag):
            """Converts POS tags from Korean treebank format to WordNet format."""
            if treebank_tag.startswith('VA') or treebank_tag.startswith('VV'):
                return 'v'  # verb
            elif treebank_tag.startswith('N'):
                return 'n'  # noun
            elif treebank_tag.startswith('M'):
                return 'r'  # adverb (ê´€í˜•ì‚¬, ë¶€ì‚¬)
            elif treebank_tag.startswith('XR') or treebank_tag.startswith('MM'):
                return 'a'  # adjective (ì–´ê·¼, ê´€í˜•ì‚¬)
            else:
                return 'n'  # default to noun

        # í•œêµ­ì–´ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™”
        if USE_KIWI:
            kiwi = Kiwi()
            print("âœ… Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš© (Java ë¶ˆí•„ìš”)")
        elif USE_KIWI == False:
            try:
                okt = Okt()
                print("âœ… KoNLPy Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©")
            except Exception as e:
                print(f"âš ï¸ KoNLPy ì´ˆê¸°í™” ì‹¤íŒ¨ (Java í™˜ê²½ ë¬¸ì œ): {e}")
                print("ğŸ’¡ í•´ê²° ë°©ë²•:")
                print("   pip install kiwipiepy  # Java ë¶ˆí•„ìš”í•œ ëŒ€ì•ˆ")
                print("   ë˜ëŠ” Java í™˜ê²½ ì„¤ì • í•„ìš”")
                raise RuntimeError("í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. kiwipiepy ë˜ëŠ” konlpyë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
            raise RuntimeError("í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # 0.1) ë¶ˆìš©ì–´ ëª©ë¡ ë¡œë“œ
        with open('data/stopwords.txt', 'r', encoding='utf-8') as f:
            stop_words = set(f.read().splitlines())

        # 0.2) ì˜ì–´ í‚¤ì›Œë“œ ë¡œë“œ â†’ ëª¨ë‘ ì†Œë¬¸ìë¡œ í†µì¼
        with open('data/eng_data.txt', 'r', encoding='utf-8') as f:
            eng_keywords = set(line.strip().lower() for line in f if line.strip())

        # 0.3) ë³µí•© í‚¤ì›Œë“œ(raw_phrases) ë¡œë“œ â†’ ì†Œë¬¸ì í‚¤, ì†Œë¬¸ì ê°’(glued)ìœ¼ë¡œ í†µì¼
        with open('data/phrase_data.txt', 'r', encoding='utf-8') as f:
            raw_phrases = [line.strip() for line in f if line.strip()]

        # ê¸¸ì´ê°€ ê¸´ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•œ ë’¤, í‚¤ì™€ ê°’ì„ ëª¨ë‘ ì†Œë¬¸ìë¡œ ë°”ê¿”ì„œ ì €ì¥
        raw_phrases.sort(key=len, reverse=True)
        concat_phrases = {
            ph.lower(): ph.replace(' ', '').lower()
            for ph in raw_phrases
        }

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 1) ì›ë³¸ â€œì²­êµ¬í•­â€ ë¦¬ìŠ¤íŠ¸(summ)ì—ì„œ ì‹¤ì œë¡œ ë“±ì¥í•œ ë³µí•©ì–´Â·ì˜ì–´ í‚¤ì›Œë“œë§Œ ì¶”ë ¤ë‚´ê¸°
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        # used_phrases : ì „ì²˜ë¦¬ ì´ì „ ì›ë¬¸ì— í¬í•¨ëœ glue(composite) ë³µí•©ì–´ ì§‘í•©
        # used_eng     : ì „ì²˜ë¦¬ ì´ì „ ì›ë¬¸ì— í¬í•¨ëœ ì˜ì–´ í‚¤ì›Œë“œ ì§‘í•©
        used_phrases = set()
        used_eng     = set()

        for summary in summ:
            if not isinstance(summary, str):
                continue

            lower = summary.lower()

            # 1.1) ë³µí•©ì–´ ë“±ì¥ ì²´í¬ (ì†Œë¬¸ì í‚¤ ê¸°ì¤€, IGNORECASE ì•ˆì „ì¥ì¹˜)
            for ph_lower, glued_lower in concat_phrases.items():
                if re.search(re.escape(ph_lower), lower, flags=re.IGNORECASE):
                    used_phrases.add(glued_lower)

            # 1.2) ì˜ì–´ í‚¤ì›Œë“œ ë“±ì¥ ì²´í¬
            for w in re.findall(r'\b[a-zA-Z]+\b', lower):
                lw = w.lower()
                if lw in eng_keywords:
                    used_eng.add(lw)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 2) ì „ì²˜ë¦¬ í•¨ìˆ˜: ë³µí•©ì–´ ì¹˜í™˜ â†’ ë¶ˆìš©ì–´ ì œê±° â†’ í˜•íƒœì†Œ ë¶„ì„ìš© í† í°í™”
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        def preprocess_patent_summaries(summaries):
            preprocessed_summaries = []

            for summary in summaries:
                if not isinstance(summary, str):
                    preprocessed_summaries.append("")
                    continue

                text = summary

                # 2.1) ë³´í˜¸í•  ë³µí•©ì–´(glued) ì¹˜í™˜ (key=ph_lower, value=glued_lower ëª¨ë‘ ì†Œë¬¸ì)
                #     â†’ IGNORECASEë¡œ ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ë©° ì¹˜í™˜
                for ph_lower, glued_lower in concat_phrases.items():
                    text = re.sub(re.escape(ph_lower), glued_lower, text, flags=re.IGNORECASE)

                # 2.2) ì¹˜í™˜ëœ ë³µí•©ì–´ë¥¼ ë“±ì¥ íšŸìˆ˜ë§Œí¼ ìˆ˜ì§‘ (ì‚¬í›„ ì²˜ë¦¬ ìœ„í•´)
                found_phrases = []
                for ph_lower, glued_lower in concat_phrases.items():
                    # textë„ ì†Œë¬¸ì ë³€í™˜í•´ì„œ ê²€ì‚¬í•˜ë©´ ì•ˆì „í•˜ì§€ë§Œ, ì´ë¯¸ ìœ„ ì¹˜í™˜ì—ì„œ í•œ ë²ˆ ì²˜ë¦¬ë¨
                    count = len(re.findall(re.escape(glued_lower), text, flags=re.IGNORECASE))
                    if count:
                        found_phrases.extend([glued_lower] * count)

                # 2.3) ì˜ì–´ í‚¤ì›Œë“œë¥¼ ë“±ì¥ íšŸìˆ˜ë§Œí¼ ìˆ˜ì§‘
                raw_eng = re.findall(r'\b[a-zA-Z]+\b', text)
                kept_eng = []
                for w in raw_eng:
                    lw = w.lower()
                    if lw in eng_keywords:
                        kept_eng.append(lw)

                # 2.4) í•œê¸€ê³¼ ê³µë°±ë§Œ ë‚¨ê¸°ê¸° (ìˆ«ìê°€ í•„ìš” ì—†ìœ¼ë©´ ì œê±°)
                kor_only = re.sub(r'[^ê°€-í£\s]', ' ', text)
                kor_only = re.sub(r'\s+', ' ', kor_only).strip()

                # 2.5) í˜•íƒœì†Œ ë¶„ì„ â†’ ë¶ˆìš©ì–´ ì œê±°
                if USE_KIWI:
                    # KiwiëŠ” tokenize() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê³  formì„ ì¶”ì¶œ
                    tokens = [token.form for token in kiwi.tokenize(kor_only)]
                else:
                    tokens = okt.morphs(kor_only, stem=True)
                tokens = [t for t in tokens if t not in stop_words]

                components = set()
                for ph_lower, glued_lower in concat_phrases.items():
                    if re.search(re.escape(glued_lower), text, flags=re.IGNORECASE):
                        # âŸ¶ ph_lowerì— ê³µë°±ì´ ìˆì—ˆë‹¤ë©´ ph_lower.split()ìœ¼ë¡œ ë¶„ë¦¬ëœ ë‹¨ì–´ë“¤ì„ componentsì— ì¶”ê°€í•˜ëŠ” ê¸°ì¡´ ë°©ì‹ ëŒ€ì‹ 
                        #     glued_lower(ê³µë°± ì—†ëŠ” í˜•íƒœ)ë§Œ componentsë¡œ ë‘ë©´, â€œì•„ì›ƒë°”ìš´ë“œâ€ ìì²´ë§Œ ì œê±° ëŒ€ìƒì´ ë¨.
                        components.add(glued_lower)

                # 2.7) componentsì— í¬í•¨ëœ í† í° ì œê±°
                tokens = [t for t in tokens if t not in components]

                # 2.8) ë³´í˜¸ëœ ë³µí•©ì–´Â·ì˜ì–´ í‚¤ì›Œë“œ íšŸìˆ˜ë§Œí¼ ì¬ì¶”ê°€
                tokens.extend(found_phrases)
                tokens.extend(kept_eng)

                preprocessed_summaries.append(" ".join(tokens))

            return preprocessed_summaries

        def extract_lemmatized_tokens(pre_docs):
            """
            pre_docs : preprocess_patent_summariesë¥¼ í†µê³¼í•œ ë¬¸ì¥ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸

            ë‚´ë¶€ì—ì„œ ì „ì—­ ë³€ìˆ˜ concat_phrases, eng_keywordsë¥¼ ì°¸ì¡°í•˜ì—¬,
            Oktê°€ ë¶„ë¦¬í•œ ë³µí•©ì–´ í•˜ìœ„ í† í° ì œê±° í›„, ë³µí•©ì–´ ì „ì²´ë§Œ ë‚¨ê¹€.
            """
            lemmatized_docs = []

            for doc in pre_docs:
                text = doc if isinstance(doc, str) else ""

                # 3.1) í˜•íƒœì†Œ íƒœê¹…
                if USE_KIWI:
                    # Kiwiì˜ ê²½ìš° pos() ë©”ì„œë“œ ì‚¬ìš©
                    tokens_with_pos = [(token.form, token.tag) for token in kiwi.tokenize(text)]
                else:
                    tokens_with_pos = okt.pos(text, stem=True)

                # 3.2) ë³µí•©ì–´(glued) ë“±ì¥ íšŸìˆ˜ë§Œí¼ ìˆ˜ì§‘
                found_phrases = []
                for ph_lower, glued_lower in concat_phrases.items():
                    count = len(re.findall(re.escape(glued_lower), text, flags=re.IGNORECASE))
                    if count:
                        found_phrases.extend([glued_lower] * count)

                # 3.3) ì¤‘ê°„ í‘œì œì–´ ìˆ˜ì§‘
                lem = []
                for token, tag in tokens_with_pos:
                    # 3.3.1) ì˜ì–´ í‚¤ì›Œë“œ(ì†Œë¬¸ì)ì¸ ê²½ìš°
                    if token.lower() in eng_keywords:
                        lem.append(token)
                        continue

                    # 3.3.2) Oktê°€ í†µì§¸ë¡œ ì¸ì‹í•œ ë³µí•©ì–´ì¸ ê²½ìš° (ì˜ˆ: "ì•„ì›ƒë°”ìš´ë“œ")
                    #        âŸ¶ concat_phrases.values()ëŠ” ëª¨ë‘ ì†Œë¬¸ì glue í˜•íƒœì„.
                    if token.lower() in concat_phrases.values():
                        lem.append(token)
                        continue

                    # 3.3.3) ì¼ë°˜ ëª…ì‚¬/ë™ì‚¬/í˜•ìš©ì‚¬/ì˜ë¬¸ ì•ŒíŒŒë²³ íƒœê·¸ì¸ ê²½ìš°
                    if wordnet_pos_tags_kor(tag):
                        lem.append(token)
                        continue

                    # ê·¸ ì™¸ëŠ” ë²„ë¦¼

                # 3.4) cleanup: ë³µí•©ì–´ ì¼ë¶€ë¡œ ë¶„ë¦¬ëœ í•˜ìœ„ í† í°(ì˜ˆ: "ì•„ì›ƒ", "ë°”ìš´ë“œ") ì œê±°
                cleaned = []
                for token in lem:
                    is_subpart = False
                    for glued_lower in found_phrases:
                        # tokenì´ glued_lower(ì˜ˆ: "ì•„ì›ƒë°”ìš´ë“œ") í•˜ìœ„ ë¬¸ìì—´ì¸ì§€ ê²€ì‚¬
                        if token.lower() in glued_lower:
                            is_subpart = True
                            break
                    if not is_subpart:
                        cleaned.append(token)

                # 3.5) ë³µí•©ì–´ ì „ì²´(glued) ë“±ì¥ íšŸìˆ˜ë§Œí¼ ë‹¤ì‹œ ì¶”ê°€
                cleaned.extend(found_phrases)

                # 3.6) ìµœì¢… ì €ì¥
                lemmatized_docs.append(" ".join(cleaned).strip())

            return lemmatized_docs

        # íŠ¹í—ˆ ìš”ì•½ë¬¸ ì „ì²˜ë¦¬
        patent_prep = preprocess_patent_summaries(summ)
        #print(patent_prep)

        # 4.4) í‘œì œì–´(lemma) ì¶”ì¶œ: ë³µí•©ì–´ ë¶„ë¦¬ ë°©ì§€ ë¡œì§
        lemmatized_patents = extract_lemmatized_tokens(patent_prep)
        #print(f'len:{len(lemmatized_patents)}')
        
        # ë°ì´í„° ê²€ì¦: ë¹ˆ ë°ì´í„° í™•ì¸
        if not lemmatized_patents:
            print("âŒ ì „ì²˜ë¦¬ëœ íŠ¹í—ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        # ë¹ˆ ë¬¸ì„œë“¤ ì œê±°
        lemmatized_patents = [doc for doc in lemmatized_patents if doc.strip()]
        
        if not lemmatized_patents:
            print("âŒ ìœ íš¨í•œ ì „ì²˜ë¦¬ëœ íŠ¹í—ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        print(f"âœ… {len(lemmatized_patents)}ê°œì˜ ì „ì²˜ë¦¬ëœ ë¬¸ì„œë¡œ í† í”½ ëª¨ë¸ë§ì„ ì§„í–‰í•©ë‹ˆë‹¤.")

        # 4.5) all_tokens ìƒì„±
        all_tokens = set()
        for doc in lemmatized_patents:
            all_tokens.update(doc.split())

        # 4.5) ìµœì¢… vocab ìƒì„±: used_phrases âˆª used_eng âˆª all_tokens â€“ stop_words
        vocab = (used_phrases | used_eng | all_tokens) - set(stop_words)

        """#### ë³´í˜¸ ë‹¨ì–´ í™•ì¸"""

        # 5.1) lowercased all_tokens ì§‘í•©
        all_tokens_lower = {tok.lower() for tok in all_tokens}

        # 5.2) eng_keywords ì§‘í•©ê³¼ êµì§‘í•© í™•ì¸
        protected_eng_in_all = all_tokens_lower.intersection(eng_keywords)

        for idx, doc in enumerate(lemmatized_patents):
            tokens = doc.split()

        seed = 42

        # Python ë‚´ì¥ ëœë¤ ëª¨ë“ˆ ì‹œë“œ ì„¤ì •
        random.seed(seed)

        # NumPy ì‹œë“œ ì„¤ì •
        np.random.seed(seed)

        # PyTorch ì‹œë“œ ì„¤ì •
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)

        # â”€â”€â”€ 0. Pretendard TTF íŒŒì¼ì„ Matplotlibì— ë“±ë¡í•˜ì—¬ í°íŠ¸ ì´ë¦„ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        import os
        import matplotlib.font_manager as fm
        # PretendardVariable.ttf íŒŒì¼ ê²½ë¡œ (ë³¸ì¸ í™˜ê²½ì— ë§ì¶° ìˆ˜ì •)
        font_path = "data/Pretendard-1.3.9/public/variable/PretendardVariable.ttf"
        if not os.path.exists(font_path):
            raise FileNotFoundError(f"Pretendard TTF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path}")
        # Matplotlib font_managerë¥¼ í†µí•´ Pretendardë¥¼ ë“±ë¡
        fm.fontManager.addfont(font_path)
        # ì‹¤ì œ í°íŠ¸ íŒ¨ë°€ë¦¬ ì´ë¦„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. Ex) "Pretendard Variable"
        pret_name = fm.FontProperties(fname=font_path).get_name()



        mpl.rcParams['font.family'] = pret_name
        mpl.rcParams['axes.unicode_minus'] = False   # í•œê¸€ ì‚¬ìš© ì‹œ ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€
        from sentence_transformers import SentenceTransformer
        # Hugging Face 429 ì—ëŸ¬ ì„ì‹œ í•´ê²°ì±… - ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
        try:
            embedding_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
        except Exception as e:
            print(f"KR-SBERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©: {e}")
            # ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©
            embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

        # lemmatized_patents: ì´ë¯¸ ì „ì²˜ë¦¬ê°€ ëë‚œ íŠ¹í—ˆ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["ë¬¸ì„œ1", "ë¬¸ì„œ2", ...])
        embeddings = embedding_model.encode(
            lemmatized_patents,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        # embeddings.shape == (ë¬¸ì„œ ìˆ˜, ì„ë² ë”© ì°¨ì›)

        # â”€â”€â”€ 3. 2D ì „ìš© UMAP íˆ¬ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        umap_2d = umap.UMAP(
            n_neighbors=15,
            n_components=2,  # 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ (ì‹œê°í™”ìš©)
            metric='cosine',
            min_dist=0.1,
            random_state=42
        )
        umap_embeddings_2d = umap_2d.fit_transform(embeddings)
        # umap_embeddings_2d.shape == (ë¬¸ì„œ ìˆ˜, 2)

        # â”€â”€â”€ 4. BERTopicìœ¼ë¡œë¶€í„° í† í”½(label) ì •ë³´ ê°€ì ¸ì˜¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ì´ë¯¸ í•™ìŠµëœ topic_model ê°ì²´ê°€ ë©”ëª¨ë¦¬ì— ì˜¬ë¼ì™€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        # ì˜ˆ: topic_model = BERTopic(...); topics, probabilities = topic_model.fit_transform(lemmatized_patents)

        topic_model = BERTopic(
            embedding_model=embedding_model,
            umap_model=umap_2d  # ê¸°ì¡´ì— ë§Œë“  umap_2d ì‚¬ìš©
        )
        #print(f'check:{lemmatized_patents}')
        topics, probabilities = topic_model.fit_transform(lemmatized_patents)
        # topics: ê¸¸ì´ == ë¬¸ì„œ ìˆ˜, ì˜ˆ) [0, 2, 1, 1, -1, 3, â€¦]

        # â”€â”€â”€ 5. ì‚¬ìš©ì ì§€ì • í† í”½ë³„ ìƒ‰ìƒ ë§¤í•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # í† í”½ 0~5ì™€ ë…¸ì´ì¦ˆ(-1)ë¥¼ ìœ„í•œ ìƒ‰ìƒ ë”•ì…”ë„ˆë¦¬
        topic_to_color = {
            0: "#8dd3c7",  # Topic 0 â†’ ì—°í•œ ì²­ë¡
            1: "#4eb3d3",  # Topic 1 â†’ ì¤‘ê°„ í†¤ íŒŒë‘
            2: "#08589e",  # Topic 2 â†’ ì§„í•œ ë‚¨ìƒ‰
            3: "#fdb462",  # Topic 3 â†’ ì—°í•œ ì£¼í™©
            4: "#fb8072",  # Topic 4 â†’ ë¶€ë“œëŸ¬ìš´ ì½”ë„ ë ˆë“œ
            5: "#b30000",  # Topic 5 â†’ ì§„í•œ ë¶‰ì€ ì£¼í™©
            -1: (0.50, 0.50, 0.50, 0.6)  # Topic -1 (ë…¸ì´ì¦ˆ) â†’ ì—°íšŒìƒ‰ íˆ¬ëª…
        }

        # ë¬¸ì„œë³„ í• ë‹¹ëœ í† í”½ ë²ˆí˜¸ë¡œ ìƒ‰ìƒ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        point_colors = [topic_to_color.get(t, 'lightgray') for t in topics]

        # â”€â”€â”€ 6. 2D UMAP ìœ„ì— í† í”½ë³„ ì‚°ì ë„ ê·¸ë¦¬ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        plt.figure(figsize=(14, 10))
        plt.scatter(
            umap_embeddings_2d[:, 0],  # xì¶• ì¢Œí‘œ
            umap_embeddings_2d[:, 1],  # yì¶• ì¢Œí‘œ
            c=point_colors,  # í† í”½ë³„ ì‚¬ìš©ì ì§€ì • ìƒ‰ìƒ
            s=70,  # ì  í¬ê¸°
            alpha=0.6,  # íˆ¬ëª…ë„
            edgecolor='none'
        )

        # â”€â”€â”€ 7. í† í”½ ì¤‘ì‹¬(centroid)ì— â€œTopic {ë²ˆí˜¸}â€ë§Œ í‘œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for t in [0, 1, 2, 3, 4, 5]:
            # í•´ë‹¹ í† í”½ tì— ì†í•œ ë¬¸ì„œë“¤ì˜ ì¸ë±ìŠ¤ë§Œ ëª¨ìë‹ˆë‹¤.
            indices = [i for i, topic_id in enumerate(topics) if topic_id == t]
            if len(indices) == 0:
                continue  # í•´ë‹¹ í† í”½ì— ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê±´ë„ˆëœë‹ˆë‹¤.

            cluster_points = umap_embeddings_2d[indices]
            centroid = cluster_points.mean(axis=0)

            # â€œTopic {ë²ˆí˜¸}â€ë§Œ í‘œì‹œ
            label_text = f"Topic {t}"

            plt.text(
                centroid[0], centroid[1], label_text,
                fontsize=12,
                weight='bold',
                color='black',
                ha='center',
                va='center',
                bbox=dict(
                    facecolor='white',
                    alpha=0.7,
                    edgecolor='gray',
                    linewidth=0.5,
                    boxstyle='round,pad=0.3'
                )
            )

        # â”€â”€â”€ 8. ë²”ë¡€(Legend) ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        legend_handles = [
            Patch(facecolor=topic_to_color[-1], edgecolor='none', label="Topic â€“1 (Noise)"),
            Patch(facecolor=topic_to_color[0], edgecolor='none', label="Topic 0"),
            Patch(facecolor=topic_to_color[1], edgecolor='none', label="Topic 1"),
            Patch(facecolor=topic_to_color[2], edgecolor='none', label="Topic 2"),
            Patch(facecolor=topic_to_color[3], edgecolor='none', label="Topic 3"),
            Patch(facecolor=topic_to_color[4], edgecolor='none', label="Topic 4"),
            Patch(facecolor=topic_to_color[5], edgecolor='none', label="Topic 5")
        ]
        plt.legend(
            handles=legend_handles,
            title="í† í”½ ë²ˆí˜¸",
            bbox_to_anchor=(1.02, 1),
            loc='upper left',
            borderaxespad=0.3,
            fontsize=11,
            title_fontsize=12
        )

        # â”€â”€â”€ 9. ì œëª© ë° ì¶• ë ˆì´ë¸” ë“± ê¾¸ë¯¸ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        plt.title(
            "UMAP 2D ë¬¸ì„œ ì„ë² ë”©ê³¼ BERTopic í† í”½ ë¶„í¬",
            fontsize=20,
            pad=20
        )
        plt.xlabel("UMAP Dimension 1", fontsize=14)
        plt.ylabel("UMAP Dimension 2", fontsize=14)

        # ì¶• ëˆˆê¸ˆ ì œê±°
        plt.xticks([])
        plt.yticks([])

        plt.tight_layout()

        # Jupyter Notebook(.ipynb) í™˜ê²½ì—ì„œëŠ” plt.show() í•œ ì¤„ë§Œìœ¼ë¡œ ì¸ë¼ì¸ ë Œë”ë§ë©ë‹ˆë‹¤.
        # plt.show()

        # â”€â”€â”€ 10. ë²¡í„° í˜•ì‹ ì €ì¥ (ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        import time
        umap_file = "umap2d_topics_custom_color_pret.png"
        
        # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê±´ë„ˆë›°ê¸°
        if os.path.exists(umap_file):
            print(f"â­ï¸ UMAP ì‹œê°í™” íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤: {umap_file}")
        else:
            print(f"ğŸ“Š UMAP ì‹œê°í™” íŒŒì¼ì„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤: {umap_file}")
            plt.savefig(umap_file, dpi=300)
            
            # íŒŒì¼ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if os.path.exists(umap_file):
                file_size = os.path.getsize(umap_file)
                print(f"âœ… UMAP ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {umap_file} ({file_size} bytes)")
            else:
                print(f"âŒ UMAP ì‹œê°í™” íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {umap_file}")
            
        plt.close()  # ë©”ëª¨ë¦¬ ì •ë¦¬
        # plt.savefig("umap2d_topics_custom_color_pret.svg", dpi=300)






        import os
        import matplotlib.font_manager as fm
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        import plotly.io as pio

        # â”€â”€â”€ 0. Pretendard TTF ë“±ë¡ (Matplotlibì„ í†µí•´ í°íŠ¸ ì´ë¦„ ì¶”ì¶œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        font_path = "data/Pretendard-1.3.9/public/variable/PretendardVariable.ttf"
        if not os.path.exists(font_path):
            raise FileNotFoundError(f"Pretendard TTF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path}")

        # Matplotlib font_managerë¥¼ ì´ìš©í•´ Pretendardë¥¼ ë“±ë¡í•œ ë’¤, ì‹¤ì œ í°íŠ¸ ì´ë¦„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        fm.fontManager.addfont(font_path)
        pret_name = fm.FontProperties(fname=font_path).get_name()
        # ì´ì œ pret_name ë³€ìˆ˜ì— ì˜ˆë¥¼ ë“¤ì–´ "Pretendard Variable" ê°™ì€ ì •í™•í•œ í°íŠ¸ íŒ¨ë°€ë¦¬ ì´ë¦„ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤.

        # â”€â”€â”€ 1. Plotly ê¸°ë³¸ í…œí”Œë¦¿ ë° í•œê¸€ í°íŠ¸ ì„¸íŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        pio.templates.default = "simple_white"
        default_font = pret_name  # â€œAppleGothicâ€ ëŒ€ì‹  â€œPretendard Variableâ€ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

        # â”€â”€â”€ 2. BERTopic ëª¨ë¸ ì¤€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ì´ë¯¸ í•™ìŠµëœ BERTopic ëª¨ë¸ ê°ì²´(topic_model)ê°€ ë©”ëª¨ë¦¬ì— ì˜¬ë¼ì™€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        # ë§Œì•½ íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì™€ì•¼ í•œë‹¤ë©´, ì•„ë˜ ë‘ ì¤„ì˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”:
        # from bertopic import BERTopic
        # topic_model = BERTopic.load("path/to/your_saved_model")

        # â”€â”€â”€ 3. í† í”½ë³„ ìƒìœ„ 12ê°œ í‚¤ì›Œë“œ(ë‹¨ì–´) ë° ì ìˆ˜ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ì´ ë¶€ë¶„ì€ ë‚˜ì¤‘ì— ì‹¤ì œ í† í”½ ëª¨ë¸ì´ ìƒì„±ëœ í›„ì— ì‹¤í–‰ë˜ë„ë¡ ì´ë™

        # â”€â”€â”€ 4. ì„œë¸Œí”Œë¡¯(2í–‰Ã—3ì—´) ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        fig = make_subplots(
            rows=2, cols=3,
            subplot_titles=[
                "Topic 0: Top 10 words",
                "Topic 1: Top 10 words",
                "Topic 2: Top 10 words",
                "Topic 3: Top 10 words",
                "Topic 4: Top 10 words",
                "Topic 5: Top 10 words",
            ],
            # 1í–‰ê³¼ 2í–‰ ì‚¬ì´ ê°„ê²©ì„ ì¶©ë¶„íˆ ë‘ê¸° ìœ„í•´ vertical_spacingì„ ì¡°ì •í•©ë‹ˆë‹¤.
            vertical_spacing=0.15,
            horizontal_spacing=0.08
        )

        # â”€â”€â”€ 5. í† í”½ë³„ ìƒ‰ìƒ (ì›í•˜ëŠ” ìƒ‰ìœ¼ë¡œ ìˆ˜ì • ê°€ëŠ¥) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        colors = {
            0: "#8dd3c7",
            1: "#4eb3d3",
            2: "#08589e",
            3: "#fdb462",
            4: "#fb8072",
            5: "#b30000"
        }

        # ì´ ë¶€ë¶„ì€ ì‹¤ì œ í† í”½ ëª¨ë¸ ìƒì„± í›„ì— ì‹¤í–‰ë˜ë„ë¡ ì´ë™ë¨
        
        
        
        
        # Vectorizer ëª¨ë¸ ì„¤ì •
        vectorizer_model = CountVectorizer(vocabulary=list(vocab), ngram_range=(1, 1), min_df=2, token_pattern=r"(?u)\b\w[\w_]+\b")  # min_df ,max_df=0.5,

        # UMAP ëª¨ë¸ ì„¤ì •
        umap_model = umap.UMAP(n_neighbors=20, n_components=10, min_dist=0.01, metric='cosine', random_state=seed) # , spread=3.0)

        # HDBSCAN ëª¨ë¸ ì„¤ì •
        hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=40, min_samples=2, metric='euclidean', cluster_selection_method='eom', prediction_data=True) #min_samples=2,

        # cTF-IDF ëª¨ë¸ ì„¤ì •
        ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=False)   #bm25_weighting=True, reduce_frequent_words=True)

        from sentence_transformers import SentenceTransformer, util
        # 2) í•œêµ­ì–´ íŠ¹í™” SBERT ë¡œë“œ
        # Hugging Face 429 ì—ëŸ¬ ì„ì‹œ í•´ê²°ì±… - ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
        try:
            embedding_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
        except Exception as e:
            print(f"KR-SBERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©: {e}")
            # ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©
            embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        # ===============================================
        # ê·¸ë¦¬ë“œì„œì¹˜ ì‹¤í–‰ ì—¬ë¶€ ì„¤ì •
        # ===============================================
        USE_GRID_SEARCH = False  # True: ê·¸ë¦¬ë“œì„œì¹˜ ì‹¤í–‰, False: ë¯¸ë¦¬ ì„¤ì •ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©
        
        # ë¯¸ë¦¬ ì„¤ì •ëœ ìµœì  íŒŒë¼ë¯¸í„° (ê·¸ë¦¬ë“œì„œì¹˜ ì—†ì´ ë°”ë¡œ ì‚¬ìš©)
        PRESET_PARAMS = {
            "n_neighbors": 35,
            "n_components": 7, 
            "min_dist": 0.5,
            "min_cluster_size": 50
        }
        
        if USE_GRID_SEARCH:
            print("ğŸ” ê·¸ë¦¬ë“œì„œì¹˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            #def grid():
            # í™˜ê²½ ë³€ìˆ˜ ë° ì‹œë“œ ì„¤ì •
            os.environ["TOKENIZERS_PARALLELISM"] = "false"
            seed = 42
            #print(f'len:{len(lemmatized_patents)}')
            # ë¬¸ì„œ í† í°í™” ì¤€ë¹„
            texts = [doc.split() for doc in lemmatized_patents]
            dictionary = Dictionary(texts)

            #í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
            # param_grid = {
            #     "n_neighbors": [10, 15, 20, 30, 40, 50],
            #     "n_components": [5, 8, 10],
            #     "min_dist": [0, 0.01],
            #     "min_cluster_size": [15, 20, 25, 30, 40, 50]
            # }

            param_grid = {
                "n_neighbors": [35],
                "n_components": [7],
                "min_dist": [0.01],
                "min_cluster_size": [40]
            }
        else:
            print("âš¡ ë¯¸ë¦¬ ì„¤ì •ëœ íŒŒë¼ë¯¸í„°ë¡œ ë°”ë¡œ ì§„í–‰í•©ë‹ˆë‹¤...")
            print(f"   - n_neighbors: {PRESET_PARAMS['n_neighbors']}")
            print(f"   - n_components: {PRESET_PARAMS['n_components']}")
            print(f"   - min_dist: {PRESET_PARAMS['min_dist']}")
            print(f"   - min_cluster_size: {PRESET_PARAMS['min_cluster_size']}")
            
            # ê·¸ë¦¬ë“œì„œì¹˜ ì—†ì´ ë°”ë¡œ BERTopic ëª¨ë¸ í›ˆë ¨ìœ¼ë¡œ ê±´ë„ˆë›°ê¸°
            # UMAP ëª¨ë¸ ì„¤ì • (ë¯¸ë¦¬ ì„¤ì •ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
            umap_model = umap.UMAP(
                n_neighbors=PRESET_PARAMS["n_neighbors"], 
                n_components=PRESET_PARAMS["n_components"], 
                min_dist=PRESET_PARAMS["min_dist"], 
                metric='cosine', 
                random_state=seed
            )
            
            # HDBSCAN ëª¨ë¸ ì„¤ì • (ë¯¸ë¦¬ ì„¤ì •ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
            hdbscan_model = hdbscan.HDBSCAN(
                min_cluster_size=PRESET_PARAMS["min_cluster_size"], 
                min_samples=2, 
                metric='euclidean', 
                cluster_selection_method='eom', 
                prediction_data=True
            )
            
            # BERTopic ëª¨ë¸ ì´ˆê¸°í™” ë° í›ˆë ¨ (ë¯¸ë¦¬ ì„¤ì •ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
            topic_model = BERTopic(
                language="korean",
                calculate_probabilities=True,
                nr_topics='auto',
                top_n_words=15,
                vectorizer_model=vectorizer_model,
                embedding_model=embedding_model,
                umap_model=umap_model,
                hdbscan_model=hdbscan_model,
                ctfidf_model=ctfidf_model,
                verbose=True
            )
            
            # ì£¼ì œ ëª¨ë¸ í›ˆë ¨
            topics, probabilities = topic_model.fit_transform(lemmatized_patents)
            
            # ì‹¤ì œ í† í”½ ëª¨ë¸ ê²°ê³¼ë¡œ ì°¨íŠ¸ ë°ì´í„° ìƒì„±
            top_n = 12
            topic_terms = {}
            topic_scores = {}
            
            all_topics = list(topic_model.get_topics().keys())
            print(f"ğŸ” ì „ì²´ í† í”½ ë²ˆí˜¸ë“¤: {all_topics}")
            
            # ì‹¤ì œ ìƒì„±ëœ í† í”½ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì°¨íŠ¸ ë°ì´í„° ìƒì„± (ìµœëŒ€ 6ê°œë¡œ ì œí•œ)
            valid_topics = sorted([t for t in all_topics if t != -1])[:6]  # ìµœëŒ€ 6ê°œë§Œ ì„ íƒ
            print(f"ğŸ” ì„ íƒëœ í† í”½ë“¤ (ìµœëŒ€ 6ê°œ): {valid_topics}")
            
            for topic_num in valid_topics:
                terms_scores = topic_model.get_topic(topic_num)  # [(term, score), ...]
                if terms_scores:
                    top_terms_scores = terms_scores[:top_n]
                    terms = [term for term, score in top_terms_scores]
                    scores = [score for term, score in top_terms_scores]
                    topic_terms[topic_num] = terms
                    topic_scores[topic_num] = scores
                    print(f"âœ… Topic {topic_num}: {len(terms)}ê°œ í‚¤ì›Œë“œ - {terms[:5]}...")
                else:
                    topic_terms[topic_num] = []
                    topic_scores[topic_num] = []
            
            # Chrome ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ matplotlibìœ¼ë¡œ ì°¨íŠ¸ ìƒì„±
            fig_mpl, axes = plt.subplots(2, 3, figsize=(15, 10))
            fig_mpl.suptitle('Topic 0~5: Top 10 words ë¶„í¬', fontsize=20, y=0.98)
            
            colors_mpl = {
                0: "#8dd3c7", 1: "#4eb3d3", 2: "#08589e", 
                3: "#fdb462", 4: "#fb8072", 5: "#b30000"
            }
            
            # ì„ íƒëœ í† í”½ë“¤ì— ë§ì¶° ì°¨íŠ¸ ìƒì„± (ìµœëŒ€ 6ê°œ)
            for i, topic_num in enumerate(valid_topics):
                row = i // 3
                col = i % 3
                ax = axes[row, col]
                
                terms = topic_terms.get(topic_num, [])
                scores = topic_scores.get(topic_num, [])
                
                if terms:
                    # ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ê°€ ìœ„ìª½ì—)
                    y_pos = range(len(terms))
                    ax.barh(y_pos, scores[::-1], color=colors_mpl.get(i, "#cccccc"), alpha=0.8)
                    ax.set_yticks(y_pos)
                    ax.set_yticklabels(terms[::-1], fontsize=10)
                    ax.set_xlabel('c-TF-IDF', fontsize=12)
                    ax.set_title(f'Topic {topic_num}: Top 10 words', fontsize=14, pad=10)
                    ax.grid(axis='x', alpha=0.3)
                else:
                    ax.set_title(f'Topic {topic_num}: No data', fontsize=14, pad=10)
                    
            plt.tight_layout()
            
            # Topic words chart íŒŒì¼ ì €ì¥ (íŒŒì¼ì´ ì—†ì„ ë•Œë§Œ ìƒì„±)
            chart_file = "topic_words_chart.png"
            
            # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê±´ë„ˆë›°ê¸°
            if os.path.exists(chart_file):
                print(f"â­ï¸ í† í”½ ì°¨íŠ¸ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤: {chart_file}")
            else:
                print(f"ğŸ“ˆ í† í”½ ì°¨íŠ¸ íŒŒì¼ì„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤: {chart_file}")
                plt.savefig(chart_file, dpi=300, bbox_inches='tight')
                
                # íŒŒì¼ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if os.path.exists(chart_file):
                    file_size = os.path.getsize(chart_file)
                    print(f"âœ… í† í”½ ì°¨íŠ¸ ì €ì¥ ì™„ë£Œ: {chart_file} ({file_size} bytes)")
                else:
                    print(f"âŒ í† í”½ ì°¨íŠ¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {chart_file}")
                
            plt.close(fig_mpl)
            
            # í† í”½ ê²°ê³¼ ì²˜ë¦¬ í›„ ë°˜í™˜ (ìµœëŒ€ 6ê°œë¡œ ì œí•œ)
            topics_dict = {}
            for topic_num in valid_topics:  # ì´ë¯¸ 6ê°œë¡œ ì œí•œëœ í† í”½ë“¤ë§Œ ì‚¬ìš©
                words = [word for word, _ in topic_model.get_topic(topic_num)]
                topics_dict[topic_num] = words

            print(f"ğŸ¯ ìµœì¢… ë°˜í™˜í•  í† í”½ ìˆ˜: {len(topics_dict)}ê°œ (ìµœëŒ€ 6ê°œë¡œ ì œí•œ)")
            return topics_dict

            # ========== ê·¸ë¦¬ë“œì„œì¹˜ ì½”ë“œ (ì£¼ì„ì²˜ë¦¬ë¨) ==========
            # ì´ ì¡°í•© ìˆ˜ ê³„ì‚°
            total_combinations = (
                len(param_grid["n_neighbors"]) *
                len(param_grid["n_components"]) *
                len(param_grid["min_dist"]) *
                len(param_grid["min_cluster_size"])
            )

            results = []

            # ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
            output_path = "../0601_grid_search_results1154.json"

            # (1) ë¹ˆ JSON íŒŒì¼ì„ ë¯¸ë¦¬ ìƒì„±í•´ ë‘ë©´, ì´í›„ ë®ì–´ì“°ê¸°ê°€ í¸í•´ì§‘ë‹ˆë‹¤.
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump({"results": [], "best": None}, f, ensure_ascii=False, indent=4)

            # Grid search with progress bar
            for grid_idx, (n_neighbors, n_components, min_dist, min_cs) in enumerate(tqdm(
                    itertools.product(
                        param_grid["n_neighbors"],
                        param_grid["n_components"],
                        param_grid["min_dist"],
                        param_grid["min_cluster_size"]
                    ),
                    total=total_combinations,
                    desc="Grid Search"
            )):
                # grid ì§„í–‰ìƒí™© ì €ì¥
                progress = {
                    "stage": "grid",
                    "current": grid_idx + 1,
                    "total": total_combinations,
                    "message": f"Grid {grid_idx + 1}/{total_combinations} ì¡°í•© í‰ê°€ ì¤‘"
                }
                with open("progress.json", "w", encoding="utf-8") as f:
                    json.dump(progress, f, ensure_ascii=False)

                # 2.1) UMAP ëª¨ë¸ ì •ì˜
                umap_model = UMAP(
                    n_neighbors=n_neighbors,
                    n_components=n_components,
                    min_dist=min_dist,
                    metric='cosine',
                    random_state=seed
                )
                # 2.2) HDBSCAN ëª¨ë¸ ì •ì˜
                hdbscan_model = HDBSCAN(
                    min_cluster_size=min_cs,
                    min_samples=2,
                    metric='euclidean',
                    cluster_selection_method='eom',
                    prediction_data=True
                )

                # 2.3) BERTopic í•™ìŠµ
                topic_model = BERTopic(
                    language="korean",
                    calculate_probabilities=True,
                    nr_topics='auto',
                    top_n_words=15,
                    vectorizer_model=vectorizer_model,
                    embedding_model=embedding_model,
                    umap_model=umap_model,
                    hdbscan_model=hdbscan_model,
                    ctfidf_model=ctfidf_model,
                    verbose=False
                )
                try:
                    topics, probs = topic_model.fit_transform(lemmatized_patents)
                except Exception as e:
                    print("âŒ fit_transform ì¤‘ ì—ëŸ¬ ë°œìƒ:", e)
                    return {}  # ë¹ˆ dict ë°˜í™˜
                #print('-------------test1-------------')

                topic_words = {
                    str(topic_id): [word for word, _ in topic_model.get_topic(topic_id)]
                    for topic_id in topic_model.get_topic_freq().Topic
                    if topic_id != -1
                }
                if len(topic_words) < 3:
                    continue

                # -------------------------------------------------------------
                # 4) Coherence ê³„ì‚°
                # -------------------------------------------------------------
                coherence_per_topic = {}
                for topic_id, words in topic_words.items():
                    cm = CoherenceModel(
                        topics=[words],
                        texts=texts,
                        dictionary=dictionary,
                        coherence='c_v'
                    )
                    coherence_per_topic[topic_id] = cm.get_coherence()
                mean_coh = sum(coherence_per_topic.values()) / len(coherence_per_topic)
                if mean_coh < 0.4:
                    continue
                #print('-------------test2-------------')
                # -------------------------------------------------------------
                # 5) í† í”½ ë¹ˆë„(Count) ì €ì¥ (ë…¸ì´ì¦ˆ í¬í•¨)
                # -------------------------------------------------------------
                topic_freq_df = topic_model.get_topic_freq()
                topic_counts = {
                    str(int(row["Topic"])): int(row["Count"])
                    for _, row in topic_freq_df.iterrows()
                }

                # -------------------------------------------------------------
                # 6) í•´ë‹¹ ì¡°í•© ê²°ê³¼ ìƒì„±
                # -------------------------------------------------------------
                current_result = {
                    "n_neighbors": n_neighbors,
                    "n_components": n_components,
                    "min_dist": min_dist,
                    "min_cluster_size": min_cs,
                    "coherence_per_topic": coherence_per_topic,
                    "mean_coherence": mean_coh,
                    "topic_counts": topic_counts,
                    "topic_words": topic_words
                }
                results.append(current_result)
                #print('-------------test3-------------')

                # -------------------------------------------------------------
                # 7) í˜„ì¬ê¹Œì§€ì˜ resultsì™€ bestë¥¼ JSON íŒŒì¼ì— ì‹¤ì‹œê°„ ë®ì–´ì“°ê¸°
                # -------------------------------------------------------------
                # ìµœì  ì¡°í•©ì„ ê³„ì‚°í•˜ë ¤ë©´ ì§€ê¸ˆê¹Œì§€ì˜ results ì¤‘ ê°€ì¥ ë†’ì€ mean_coherenceë¥¼ ì°¾ìŠµë‹ˆë‹¤.
                best_so_far = max(results, key=lambda x: x["mean_coherence"])
                output_dict = {"results": results, "best": best_so_far}

                with open(output_path, "w", encoding="utf-8") as f:
                    json.dump(output_dict, f, ensure_ascii=False, indent=4)

            best = max(results, key=lambda x: x["mean_coherence"]) if results else None
            if best:
                print(
                    f"Grid search ì™„ë£Œ. ìµœì  ì¡°í•©: "
                    f"{best['n_neighbors']}, {best['n_components']}, "
                    f"{best['min_dist']}, {best['min_cluster_size']} "
                    f"(í‰ê·  Coherence: {best['mean_coherence']:.4f})"
                )
            else:
                print("ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # BERTopic ëª¨ë¸ ì´ˆê¸°í™” ë° í›ˆë ¨ (ê·¸ë¦¬ë“œì„œì¹˜ í›„)
            topic_model = BERTopic(
                language="korean",  # ì–¸ì–´ ì„¤ì •
                calculate_probabilities=True,  # í™•ë¥  ê³„ì‚° ì—¬ë¶€
                nr_topics='auto',  # ì£¼ì œì˜ ìˆ˜ ì œí•œ, autoë¡œ í•´ì•¼ HDBSCAN ì‘ë™
                top_n_words=15,  # ê° ì£¼ì œì˜ ìƒìœ„ ë‹¨ì–´ ìˆ˜
                # ì£¼ì œì˜ ìµœì†Œ í¬ê¸°
                vectorizer_model=vectorizer_model,  # ë²¡í„°í™” ëª¨ë¸
                embedding_model=embedding_model,  # ì„ë² ë”© ëª¨ë¸
                umap_model=umap_model,  # UMAP ëª¨ë¸
                hdbscan_model=hdbscan_model,  # HDBSCAN ëª¨ë¸
                ctfidf_model=ctfidf_model,  # c-TFIDF ëª¨ë¸
                #representation_model=representation_model,
                verbose=True  # ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€
            )

            # ì£¼ì œ ëª¨ë¸ í›ˆë ¨
            #print(f'check:{lemmatized_patents}')
            topics, probabilities = topic_model.fit_transform(lemmatized_patents)
            
            #-----------------GTM1-----------------
            #------------------------------------------------------------------
            
            # í† í”½ ë²ˆí˜¸ë¥¼ ê°€ì ¸ì˜¤ê³  ì²« ë²ˆì§¸ í† í”½(-1)ì„ ì œì™¸
            topics_dict = {}
            all_topics = list(topic_model.get_topics().keys())
            print(f"ğŸ” ì „ì²´ í† í”½ ë²ˆí˜¸ë“¤: {all_topics}")
            
            # -1 í† í”½(ë…¸ì´ì¦ˆ) ì œì™¸í•˜ê³  ìµœëŒ€ 6ê°œ í† í”½ë§Œ í¬í•¨
            valid_topics = sorted([t for t in all_topics if t != -1])[:6]  # ìµœëŒ€ 6ê°œë¡œ ì œí•œ
            print(f"ğŸ” ì„ íƒëœ í† í”½ë“¤ (ìµœëŒ€ 6ê°œ): {valid_topics}")
            
            for topic_num in valid_topics:
                words = [word for word, _ in topic_model.get_topic(topic_num)]
                topics_dict[topic_num] = words
                print(f"âœ… Topic {topic_num}: {len(words)}ê°œ í‚¤ì›Œë“œ - {words[:5]}...")

            print(f"ğŸ¯ ìµœì¢… ë°˜í™˜í•  í† í”½ ìˆ˜: {len(topics_dict)}ê°œ (ìµœëŒ€ 6ê°œë¡œ ì œí•œ)")
            return topics_dict
#------------------------------------------------------------------------                       
    