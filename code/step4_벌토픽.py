import os
os.environ["USE_TF"] = "0"
os.environ["USE_TORCH"] = "1"

# Compatibility patch for scipy.linalg.triu issue
import numpy as np
import scipy.linalg
if not hasattr(scipy.linalg, 'triu'):
    scipy.linalg.triu = np.triu

# Compatibility patch for huggingface_hub
try:
    from huggingface_hub import cached_download
except ImportError:
    from huggingface_hub import hf_hub_download as cached_download
    import huggingface_hub
    huggingface_hub.cached_download = cached_download

from tqdm import tqdm
import itertools
from umap import UMAP
from hdbscan import HDBSCAN
from bertopic import BERTopic
from gensim.corpora import Dictionary
from gensim.models.coherencemodel import CoherenceModel
import os
import json
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib.patches import Patch
from bertopic import BERTopic
import umap.umap_ as umap
import torch
import random
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm, tqdm_notebook
from torch.optim import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup
from kobert_transformers import get_kobert_model
import nltk
from itertools import product
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk import pos_tag
from sklearn.preprocessing import normalize
import pandas as pd
# Java ë¶ˆí•„ìš”í•œ í•œêµ­ì–´ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
try:
    from kiwipiepy import Kiwi
    USE_KIWI = True
except ImportError:
    try:
        from konlpy.tag import Okt
        USE_KIWI = False
    except ImportError:
        USE_KIWI = None
from transformers import BertModel, BertTokenizer
from transformers import AutoModel
from bertopic import BERTopic
from bertopic.vectorizers import ClassTfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
try:
    from ugtm import eGTM
except ImportError:
    eGTM = None
    print("Warning: ugtm not available due to compatibility issues")
import umap
import hdbscan
import re

class Step4:
    def ber(self):
        nltk.download('punkt')
        nltk.download('wordnet')
        nltk.download('stopwords')
        nltk.download('averaged_perceptron_tagger')

        file_path = './extract_end.csv'
        patent = pd.read_csv(file_path)

        summ = patent['astrtCont']+patent['ë°œëª…ëª…ì¹­']
        #summ=patent['ì²­êµ¬í•­']
        #print(summ)
        # í‘œì œì–´ ì¶”ì¶œê¸° ì„¤ì •
        lemmatizer = WordNetLemmatizer()


        def wordnet_pos_tags_kor(treebank_tag):
            """Converts POS tags from Korean treebank format to WordNet format."""
            if treebank_tag.startswith('VA') or treebank_tag.startswith('VV'):
                return 'v'  # verb
            elif treebank_tag.startswith('N'):
                return 'n'  # noun
            elif treebank_tag.startswith('M'):
                return 'r'  # adverb (ê´€í˜•ì‚¬, ë¶€ì‚¬)
            elif treebank_tag.startswith('XR') or treebank_tag.startswith('MM'):
                return 'a'  # adjective (ì–´ê·¼, ê´€í˜•ì‚¬)
            else:
                return 'n'  # default to noun

        # í•œêµ­ì–´ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™”
        if USE_KIWI:
            kiwi = Kiwi()
            print("âœ… Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš© (Java ë¶ˆí•„ìš”)")
        elif USE_KIWI == False:
            try:
                okt = Okt()
                print("âœ… KoNLPy Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©")
            except Exception as e:
                print(f"âš ï¸ KoNLPy ì´ˆê¸°í™” ì‹¤íŒ¨ (Java í™˜ê²½ ë¬¸ì œ): {e}")
                print("ğŸ’¡ í•´ê²° ë°©ë²•:")
                print("   pip install kiwipiepy  # Java ë¶ˆí•„ìš”í•œ ëŒ€ì•ˆ")
                print("   ë˜ëŠ” Java í™˜ê²½ ì„¤ì • í•„ìš”")
                raise RuntimeError("í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. kiwipiepy ë˜ëŠ” konlpyë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
            raise RuntimeError("í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # 0.1) ë¶ˆìš©ì–´ ëª©ë¡ ë¡œë“œ
        with open('data/stopwords.txt', 'r', encoding='utf-8') as f:
            stop_words = set(f.read().splitlines())

        # 0.2) ì˜ì–´ í‚¤ì›Œë“œ ë¡œë“œ â†’ ëª¨ë‘ ì†Œë¬¸ìë¡œ í†µì¼
        with open('data/eng_data.txt', 'r', encoding='utf-8') as f:
            eng_keywords = set(line.strip().lower() for line in f if line.strip())

        # 0.3) ë³µí•© í‚¤ì›Œë“œ(raw_phrases) ë¡œë“œ â†’ ì†Œë¬¸ì í‚¤, ì†Œë¬¸ì ê°’(glued)ìœ¼ë¡œ í†µì¼
        with open('data/phrase_data.txt', 'r', encoding='utf-8') as f:
            raw_phrases = [line.strip() for line in f if line.strip()]

        # ê¸¸ì´ê°€ ê¸´ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•œ ë’¤, í‚¤ì™€ ê°’ì„ ëª¨ë‘ ì†Œë¬¸ìë¡œ ë°”ê¿”ì„œ ì €ì¥
        raw_phrases.sort(key=len, reverse=True)
        concat_phrases = {
            ph.lower(): ph.replace(' ', '').lower()
            for ph in raw_phrases
        }

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 1) ì›ë³¸ â€œì²­êµ¬í•­â€ ë¦¬ìŠ¤íŠ¸(summ)ì—ì„œ ì‹¤ì œë¡œ ë“±ì¥í•œ ë³µí•©ì–´Â·ì˜ì–´ í‚¤ì›Œë“œë§Œ ì¶”ë ¤ë‚´ê¸°
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        # used_phrases : ì „ì²˜ë¦¬ ì´ì „ ì›ë¬¸ì— í¬í•¨ëœ glue(composite) ë³µí•©ì–´ ì§‘í•©
        # used_eng     : ì „ì²˜ë¦¬ ì´ì „ ì›ë¬¸ì— í¬í•¨ëœ ì˜ì–´ í‚¤ì›Œë“œ ì§‘í•©
        used_phrases = set()
        used_eng     = set()

        for summary in summ:
            if not isinstance(summary, str):
                continue

            lower = summary.lower()

            # 1.1) ë³µí•©ì–´ ë“±ì¥ ì²´í¬ (ì†Œë¬¸ì í‚¤ ê¸°ì¤€, IGNORECASE ì•ˆì „ì¥ì¹˜)
            for ph_lower, glued_lower in concat_phrases.items():
                if re.search(re.escape(ph_lower), lower, flags=re.IGNORECASE):
                    used_phrases.add(glued_lower)

            # 1.2) ì˜ì–´ í‚¤ì›Œë“œ ë“±ì¥ ì²´í¬
            for w in re.findall(r'\b[a-zA-Z]+\b', lower):
                lw = w.lower()
                if lw in eng_keywords:
                    used_eng.add(lw)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 2) ì „ì²˜ë¦¬ í•¨ìˆ˜: ë³µí•©ì–´ ì¹˜í™˜ â†’ ë¶ˆìš©ì–´ ì œê±° â†’ í˜•íƒœì†Œ ë¶„ì„ìš© í† í°í™”
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        def preprocess_patent_summaries(summaries):
            preprocessed_summaries = []

            for summary in summaries:
                if not isinstance(summary, str):
                    preprocessed_summaries.append("")
                    continue

                text = summary

                # 2.1) ë³´í˜¸í•  ë³µí•©ì–´(glued) ì¹˜í™˜ (key=ph_lower, value=glued_lower ëª¨ë‘ ì†Œë¬¸ì)
                #     â†’ IGNORECASEë¡œ ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ë©° ì¹˜í™˜
                for ph_lower, glued_lower in concat_phrases.items():
                    text = re.sub(re.escape(ph_lower), glued_lower, text, flags=re.IGNORECASE)

                # 2.2) ì¹˜í™˜ëœ ë³µí•©ì–´ë¥¼ ë“±ì¥ íšŸìˆ˜ë§Œí¼ ìˆ˜ì§‘ (ì‚¬í›„ ì²˜ë¦¬ ìœ„í•´)
                found_phrases = []
                for ph_lower, glued_lower in concat_phrases.items():
                    # textë„ ì†Œë¬¸ì ë³€í™˜í•´ì„œ ê²€ì‚¬í•˜ë©´ ì•ˆì „í•˜ì§€ë§Œ, ì´ë¯¸ ìœ„ ì¹˜í™˜ì—ì„œ í•œ ë²ˆ ì²˜ë¦¬ë¨
                    count = len(re.findall(re.escape(glued_lower), text, flags=re.IGNORECASE))
                    if count:
                        found_phrases.extend([glued_lower] * count)

                # 2.3) ì˜ì–´ í‚¤ì›Œë“œë¥¼ ë“±ì¥ íšŸìˆ˜ë§Œí¼ ìˆ˜ì§‘
                raw_eng = re.findall(r'\b[a-zA-Z]+\b', text)
                kept_eng = []
                for w in raw_eng:
                    lw = w.lower()
                    if lw in eng_keywords:
                        kept_eng.append(lw)

                # 2.4) í•œê¸€ê³¼ ê³µë°±ë§Œ ë‚¨ê¸°ê¸° (ìˆ«ìê°€ í•„ìš” ì—†ìœ¼ë©´ ì œê±°)
                kor_only = re.sub(r'[^ê°€-í£\s]', ' ', text)
                kor_only = re.sub(r'\s+', ' ', kor_only).strip()

                # 2.5) í˜•íƒœì†Œ ë¶„ì„ â†’ ë¶ˆìš©ì–´ ì œê±°
                if USE_KIWI:
                    # KiwiëŠ” tokenize() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê³  formì„ ì¶”ì¶œ
                    tokens = [token.form for token in kiwi.tokenize(kor_only)]
                else:
                    tokens = okt.morphs(kor_only, stem=True)
                tokens = [t for t in tokens if t not in stop_words]

                components = set()
                for ph_lower, glued_lower in concat_phrases.items():
                    if re.search(re.escape(glued_lower), text, flags=re.IGNORECASE):
                        # âŸ¶ ph_lowerì— ê³µë°±ì´ ìˆì—ˆë‹¤ë©´ ph_lower.split()ìœ¼ë¡œ ë¶„ë¦¬ëœ ë‹¨ì–´ë“¤ì„ componentsì— ì¶”ê°€í•˜ëŠ” ê¸°ì¡´ ë°©ì‹ ëŒ€ì‹ 
                        #     glued_lower(ê³µë°± ì—†ëŠ” í˜•íƒœ)ë§Œ componentsë¡œ ë‘ë©´, â€œì•„ì›ƒë°”ìš´ë“œâ€ ìì²´ë§Œ ì œê±° ëŒ€ìƒì´ ë¨.
                        components.add(glued_lower)

                # 2.7) componentsì— í¬í•¨ëœ í† í° ì œê±°
                tokens = [t for t in tokens if t not in components]

                # 2.8) ë³´í˜¸ëœ ë³µí•©ì–´Â·ì˜ì–´ í‚¤ì›Œë“œ íšŸìˆ˜ë§Œí¼ ì¬ì¶”ê°€
                tokens.extend(found_phrases)
                tokens.extend(kept_eng)

                preprocessed_summaries.append(" ".join(tokens))

            return preprocessed_summaries

        def extract_lemmatized_tokens(pre_docs):
            """
            pre_docs : preprocess_patent_summariesë¥¼ í†µê³¼í•œ ë¬¸ì¥ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸

            ë‚´ë¶€ì—ì„œ ì „ì—­ ë³€ìˆ˜ concat_phrases, eng_keywordsë¥¼ ì°¸ì¡°í•˜ì—¬,
            Oktê°€ ë¶„ë¦¬í•œ ë³µí•©ì–´ í•˜ìœ„ í† í° ì œê±° í›„, ë³µí•©ì–´ ì „ì²´ë§Œ ë‚¨ê¹€.
            """
            lemmatized_docs = []

            for doc in pre_docs:
                text = doc if isinstance(doc, str) else ""

                # 3.1) í˜•íƒœì†Œ íƒœê¹…
                if USE_KIWI:
                    # Kiwiì˜ ê²½ìš° pos() ë©”ì„œë“œ ì‚¬ìš©
                    tokens_with_pos = [(token.form, token.tag) for token in kiwi.tokenize(text)]
                else:
                    tokens_with_pos = okt.pos(text, stem=True)

                # 3.2) ë³µí•©ì–´(glued) ë“±ì¥ íšŸìˆ˜ë§Œí¼ ìˆ˜ì§‘
                found_phrases = []
                for ph_lower, glued_lower in concat_phrases.items():
                    count = len(re.findall(re.escape(glued_lower), text, flags=re.IGNORECASE))
                    if count:
                        found_phrases.extend([glued_lower] * count)

                # 3.3) ì¤‘ê°„ í‘œì œì–´ ìˆ˜ì§‘
                lem = []
                for token, tag in tokens_with_pos:
                    # 3.3.1) ì˜ì–´ í‚¤ì›Œë“œ(ì†Œë¬¸ì)ì¸ ê²½ìš°
                    if token.lower() in eng_keywords:
                        lem.append(token)
                        continue

                    # 3.3.2) Oktê°€ í†µì§¸ë¡œ ì¸ì‹í•œ ë³µí•©ì–´ì¸ ê²½ìš° (ì˜ˆ: "ì•„ì›ƒë°”ìš´ë“œ")
                    #        âŸ¶ concat_phrases.values()ëŠ” ëª¨ë‘ ì†Œë¬¸ì glue í˜•íƒœì„.
                    if token.lower() in concat_phrases.values():
                        lem.append(token)
                        continue

                    # 3.3.3) ì¼ë°˜ ëª…ì‚¬/ë™ì‚¬/í˜•ìš©ì‚¬/ì˜ë¬¸ ì•ŒíŒŒë²³ íƒœê·¸ì¸ ê²½ìš°
                    if wordnet_pos_tags_kor(tag):
                        lem.append(token)
                        continue

                    # ê·¸ ì™¸ëŠ” ë²„ë¦¼

                # 3.4) cleanup: ë³µí•©ì–´ ì¼ë¶€ë¡œ ë¶„ë¦¬ëœ í•˜ìœ„ í† í°(ì˜ˆ: "ì•„ì›ƒ", "ë°”ìš´ë“œ") ì œê±°
                cleaned = []
                for token in lem:
                    is_subpart = False
                    for glued_lower in found_phrases:
                        # tokenì´ glued_lower(ì˜ˆ: "ì•„ì›ƒë°”ìš´ë“œ") í•˜ìœ„ ë¬¸ìì—´ì¸ì§€ ê²€ì‚¬
                        if token.lower() in glued_lower:
                            is_subpart = True
                            break
                    if not is_subpart:
                        cleaned.append(token)

                # 3.5) ë³µí•©ì–´ ì „ì²´(glued) ë“±ì¥ íšŸìˆ˜ë§Œí¼ ë‹¤ì‹œ ì¶”ê°€
                cleaned.extend(found_phrases)

                # 3.6) ìµœì¢… ì €ì¥
                lemmatized_docs.append(" ".join(cleaned).strip())

            return lemmatized_docs

        # íŠ¹í—ˆ ìš”ì•½ë¬¸ ì „ì²˜ë¦¬
        patent_prep = preprocess_patent_summaries(summ)
        #print(patent_prep)

        # 4.4) í‘œì œì–´(lemma) ì¶”ì¶œ: ë³µí•©ì–´ ë¶„ë¦¬ ë°©ì§€ ë¡œì§
        lemmatized_patents = extract_lemmatized_tokens(patent_prep)
        #print(f'len:{len(lemmatized_patents)}')
        
        # ë°ì´í„° ê²€ì¦: ë¹ˆ ë°ì´í„° í™•ì¸
        if not lemmatized_patents:
            print("âŒ ì „ì²˜ë¦¬ëœ íŠ¹í—ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        # ë¹ˆ ë¬¸ì„œë“¤ ì œê±°
        lemmatized_patents = [doc for doc in lemmatized_patents if doc.strip()]
        
        if not lemmatized_patents:
            print("âŒ ìœ íš¨í•œ ì „ì²˜ë¦¬ëœ íŠ¹í—ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        print(f"âœ… {len(lemmatized_patents)}ê°œì˜ ì „ì²˜ë¦¬ëœ ë¬¸ì„œë¡œ í† í”½ ëª¨ë¸ë§ì„ ì§„í–‰í•©ë‹ˆë‹¤.")

        # 4.5) all_tokens ìƒì„±
        all_tokens = set()
        for doc in lemmatized_patents:
            all_tokens.update(doc.split())

        # 4.5) ìµœì¢… vocab ìƒì„±: used_phrases âˆª used_eng âˆª all_tokens â€“ stop_words
        vocab = (used_phrases | used_eng | all_tokens) - set(stop_words)

        """#### ë³´í˜¸ ë‹¨ì–´ í™•ì¸"""

        # 5.1) lowercased all_tokens ì§‘í•©
        all_tokens_lower = {tok.lower() for tok in all_tokens}

        # 5.2) eng_keywords ì§‘í•©ê³¼ êµì§‘í•© í™•ì¸
        protected_eng_in_all = all_tokens_lower.intersection(eng_keywords)

        for idx, doc in enumerate(lemmatized_patents):
            tokens = doc.split()

        seed = 42

        # Python ë‚´ì¥ ëœë¤ ëª¨ë“ˆ ì‹œë“œ ì„¤ì •
        random.seed(seed)

        # NumPy ì‹œë“œ ì„¤ì •
        np.random.seed(seed)

        # PyTorch ì‹œë“œ ì„¤ì •
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)

        # â”€â”€â”€ 0. Pretendard TTF íŒŒì¼ì„ Matplotlibì— ë“±ë¡í•˜ì—¬ í°íŠ¸ ì´ë¦„ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        import os
        import matplotlib.font_manager as fm
        # PretendardVariable.ttf íŒŒì¼ ê²½ë¡œ (ë³¸ì¸ í™˜ê²½ì— ë§ì¶° ìˆ˜ì •)
        font_path = "data/Pretendard-1.3.9/public/variable/PretendardVariable.ttf"
        if not os.path.exists(font_path):
            raise FileNotFoundError(f"Pretendard TTF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path}")
        # Matplotlib font_managerë¥¼ í†µí•´ Pretendardë¥¼ ë“±ë¡
        fm.fontManager.addfont(font_path)
        # ì‹¤ì œ í°íŠ¸ íŒ¨ë°€ë¦¬ ì´ë¦„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. Ex) "Pretendard Variable"
        pret_name = fm.FontProperties(fname=font_path).get_name()



        mpl.rcParams['font.family'] = pret_name
        mpl.rcParams['axes.unicode_minus'] = False   # í•œê¸€ ì‚¬ìš© ì‹œ ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€
        from sentence_transformers import SentenceTransformer
        embedding_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")

        # lemmatized_patents: ì´ë¯¸ ì „ì²˜ë¦¬ê°€ ëë‚œ íŠ¹í—ˆ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["ë¬¸ì„œ1", "ë¬¸ì„œ2", ...])
        embeddings = embedding_model.encode(
            lemmatized_patents,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        # embeddings.shape == (ë¬¸ì„œ ìˆ˜, ì„ë² ë”© ì°¨ì›)

        # â”€â”€â”€ 3. 2D ì „ìš© UMAP íˆ¬ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        umap_2d = umap.UMAP(
            n_neighbors=15,
            n_components=2,  # 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ (ì‹œê°í™”ìš©)
            metric='cosine',
            min_dist=0.1,
            random_state=42
        )
        umap_embeddings_2d = umap_2d.fit_transform(embeddings)
        # umap_embeddings_2d.shape == (ë¬¸ì„œ ìˆ˜, 2)

        # â”€â”€â”€ 4. BERTopicìœ¼ë¡œë¶€í„° í† í”½(label) ì •ë³´ ê°€ì ¸ì˜¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ì´ë¯¸ í•™ìŠµëœ topic_model ê°ì²´ê°€ ë©”ëª¨ë¦¬ì— ì˜¬ë¼ì™€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        # ì˜ˆ: topic_model = BERTopic(...); topics, probabilities = topic_model.fit_transform(lemmatized_patents)

        topic_model = BERTopic(
            embedding_model=embedding_model,
            umap_model=umap_2d  # ê¸°ì¡´ì— ë§Œë“  umap_2d ì‚¬ìš©
        )
        #print(f'check:{lemmatized_patents}')
        topics, probabilities = topic_model.fit_transform(lemmatized_patents)
        # topics: ê¸¸ì´ == ë¬¸ì„œ ìˆ˜, ì˜ˆ) [0, 2, 1, 1, -1, 3, â€¦]

        # â”€â”€â”€ 5. ì‚¬ìš©ì ì§€ì • í† í”½ë³„ ìƒ‰ìƒ ë§¤í•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # í† í”½ 0~5ì™€ ë…¸ì´ì¦ˆ(-1)ë¥¼ ìœ„í•œ ìƒ‰ìƒ ë”•ì…”ë„ˆë¦¬
        topic_to_color = {
            0: "#8dd3c7",  # Topic 0 â†’ ì—°í•œ ì²­ë¡
            1: "#4eb3d3",  # Topic 1 â†’ ì¤‘ê°„ í†¤ íŒŒë‘
            2: "#08589e",  # Topic 2 â†’ ì§„í•œ ë‚¨ìƒ‰
            3: "#fdb462",  # Topic 3 â†’ ì—°í•œ ì£¼í™©
            4: "#fb8072",  # Topic 4 â†’ ë¶€ë“œëŸ¬ìš´ ì½”ë„ ë ˆë“œ
            5: "#b30000",  # Topic 5 â†’ ì§„í•œ ë¶‰ì€ ì£¼í™©
            -1: (0.50, 0.50, 0.50, 0.6)  # Topic -1 (ë…¸ì´ì¦ˆ) â†’ ì—°íšŒìƒ‰ íˆ¬ëª…
        }

        # ë¬¸ì„œë³„ í• ë‹¹ëœ í† í”½ ë²ˆí˜¸ë¡œ ìƒ‰ìƒ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        point_colors = [topic_to_color.get(t, 'lightgray') for t in topics]

        # â”€â”€â”€ 6. 2D UMAP ìœ„ì— í† í”½ë³„ ì‚°ì ë„ ê·¸ë¦¬ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        plt.figure(figsize=(14, 10))
        plt.scatter(
            umap_embeddings_2d[:, 0],  # xì¶• ì¢Œí‘œ
            umap_embeddings_2d[:, 1],  # yì¶• ì¢Œí‘œ
            c=point_colors,  # í† í”½ë³„ ì‚¬ìš©ì ì§€ì • ìƒ‰ìƒ
            s=70,  # ì  í¬ê¸°
            alpha=0.6,  # íˆ¬ëª…ë„
            edgecolor='none'
        )

        # â”€â”€â”€ 7. í† í”½ ì¤‘ì‹¬(centroid)ì— â€œTopic {ë²ˆí˜¸}â€ë§Œ í‘œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for t in [0, 1, 2, 3, 4, 5]:
            # í•´ë‹¹ í† í”½ tì— ì†í•œ ë¬¸ì„œë“¤ì˜ ì¸ë±ìŠ¤ë§Œ ëª¨ìë‹ˆë‹¤.
            indices = [i for i, topic_id in enumerate(topics) if topic_id == t]
            if len(indices) == 0:
                continue  # í•´ë‹¹ í† í”½ì— ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê±´ë„ˆëœë‹ˆë‹¤.

            cluster_points = umap_embeddings_2d[indices]
            centroid = cluster_points.mean(axis=0)

            # â€œTopic {ë²ˆí˜¸}â€ë§Œ í‘œì‹œ
            label_text = f"Topic {t}"

            plt.text(
                centroid[0], centroid[1], label_text,
                fontsize=12,
                weight='bold',
                color='black',
                ha='center',
                va='center',
                bbox=dict(
                    facecolor='white',
                    alpha=0.7,
                    edgecolor='gray',
                    linewidth=0.5,
                    boxstyle='round,pad=0.3'
                )
            )

        # â”€â”€â”€ 8. ë²”ë¡€(Legend) ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        legend_handles = [
            Patch(facecolor=topic_to_color[-1], edgecolor='none', label="Topic â€“1 (Noise)"),
            Patch(facecolor=topic_to_color[0], edgecolor='none', label="Topic 0"),
            Patch(facecolor=topic_to_color[1], edgecolor='none', label="Topic 1"),
            Patch(facecolor=topic_to_color[2], edgecolor='none', label="Topic 2"),
            Patch(facecolor=topic_to_color[3], edgecolor='none', label="Topic 3"),
            Patch(facecolor=topic_to_color[4], edgecolor='none', label="Topic 4"),
            Patch(facecolor=topic_to_color[5], edgecolor='none', label="Topic 5")
        ]
        plt.legend(
            handles=legend_handles,
            title="í† í”½ ë²ˆí˜¸",
            bbox_to_anchor=(1.02, 1),
            loc='upper left',
            borderaxespad=0.3,
            fontsize=11,
            title_fontsize=12
        )

        # â”€â”€â”€ 9. ì œëª© ë° ì¶• ë ˆì´ë¸” ë“± ê¾¸ë¯¸ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        plt.title(
            "UMAP 2D ë¬¸ì„œ ì„ë² ë”©ê³¼ BERTopic í† í”½ ë¶„í¬",
            fontsize=20,
            pad=20
        )
        plt.xlabel("UMAP Dimension 1", fontsize=14)
        plt.ylabel("UMAP Dimension 2", fontsize=14)

        # ì¶• ëˆˆê¸ˆ ì œê±°
        plt.xticks([])
        plt.yticks([])

        plt.tight_layout()

        # Jupyter Notebook(.ipynb) í™˜ê²½ì—ì„œëŠ” plt.show() í•œ ì¤„ë§Œìœ¼ë¡œ ì¸ë¼ì¸ ë Œë”ë§ë©ë‹ˆë‹¤.
        # plt.show()

        # â”€â”€â”€ 10. ë²¡í„° í˜•ì‹ ì €ì¥ (ì„ íƒ ì‚¬í•­) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        plt.savefig("umap2d_topics_custom_color_pret.png", dpi=300)
        # plt.savefig("umap2d_topics_custom_color_pret.svg", dpi=300)

        import os
        import matplotlib.font_manager as fm
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        import plotly.io as pio

        # â”€â”€â”€ 0. Pretendard TTF ë“±ë¡ (Matplotlibì„ í†µí•´ í°íŠ¸ ì´ë¦„ ì¶”ì¶œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        font_path = "data/Pretendard-1.3.9/public/variable/PretendardVariable.ttf"
        if not os.path.exists(font_path):
            raise FileNotFoundError(f"Pretendard TTF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path}")

        # Matplotlib font_managerë¥¼ ì´ìš©í•´ Pretendardë¥¼ ë“±ë¡í•œ ë’¤, ì‹¤ì œ í°íŠ¸ ì´ë¦„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        fm.fontManager.addfont(font_path)
        pret_name = fm.FontProperties(fname=font_path).get_name()
        # ì´ì œ pret_name ë³€ìˆ˜ì— ì˜ˆë¥¼ ë“¤ì–´ "Pretendard Variable" ê°™ì€ ì •í™•í•œ í°íŠ¸ íŒ¨ë°€ë¦¬ ì´ë¦„ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤.

        # â”€â”€â”€ 1. Plotly ê¸°ë³¸ í…œí”Œë¦¿ ë° í•œê¸€ í°íŠ¸ ì„¸íŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        pio.templates.default = "simple_white"
        default_font = pret_name  # â€œAppleGothicâ€ ëŒ€ì‹  â€œPretendard Variableâ€ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

        # â”€â”€â”€ 2. BERTopic ëª¨ë¸ ì¤€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ì´ë¯¸ í•™ìŠµëœ BERTopic ëª¨ë¸ ê°ì²´(topic_model)ê°€ ë©”ëª¨ë¦¬ì— ì˜¬ë¼ì™€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        # ë§Œì•½ íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì™€ì•¼ í•œë‹¤ë©´, ì•„ë˜ ë‘ ì¤„ì˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”:
        # from bertopic import BERTopic
        # topic_model = BERTopic.load("path/to/your_saved_model")

        # â”€â”€â”€ 3. í† í”½ë³„ ìƒìœ„ 12ê°œ í‚¤ì›Œë“œ(ë‹¨ì–´) ë° ì ìˆ˜ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        top_n = 12
        topic_terms = {}
        topic_scores = {}

        for t in range(6):  # í† í”½ 0~5
            terms_scores = topic_model.get_topic(t)  # [(term, score), ...]
            if not terms_scores:
                topic_terms[t] = []
                topic_scores[t] = []
                continue
            top_terms_scores = terms_scores[:top_n]
            terms = [term for term, score in top_terms_scores]
            scores = [score for term, score in top_terms_scores]
            topic_terms[t] = terms
            topic_scores[t] = scores

        # â”€â”€â”€ 4. ì„œë¸Œí”Œë¡¯(2í–‰Ã—3ì—´) ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        fig = make_subplots(
            rows=2, cols=3,
            subplot_titles=[
                "Topic 0: Top 12 words",
                "Topic 1: Top 12 words",
                "Topic 2: Top 12 words",
                "Topic 3: Top 12 words",
                "Topic 4: Top 12 words",
                "Topic 5: Top 12 words",
            ],
            # 1í–‰ê³¼ 2í–‰ ì‚¬ì´ ê°„ê²©ì„ ì¶©ë¶„íˆ ë‘ê¸° ìœ„í•´ vertical_spacingì„ ì¡°ì •í•©ë‹ˆë‹¤.
            vertical_spacing=0.15,
            horizontal_spacing=0.08
        )

        # â”€â”€â”€ 5. í† í”½ë³„ ìƒ‰ìƒ (ì›í•˜ëŠ” ìƒ‰ìœ¼ë¡œ ìˆ˜ì • ê°€ëŠ¥) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        colors = {
            0: "#8dd3c7",
            1: "#4eb3d3",
            2: "#08589e",
            3: "#fdb462",
            4: "#fb8072",
            5: "#b30000"
        }

        # â”€â”€â”€ 6. ê° í† í”½ ì„œë¸Œí”Œë¡¯ì— Bar ì°¨íŠ¸ ì¶”ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for t in range(6):
            idx = t
            row = idx // 3 + 1  # 0â†’1, 1â†’1, 2â†’1, 3â†’2, 4â†’2, 5â†’2
            col = idx % 3 + 1  # 0â†’1, 1â†’2, 2â†’3, 3â†’1, 4â†’2, 5â†’3

            terms = topic_terms[t]
            scores = topic_scores[t]
            if not terms:
                continue

            fig.add_trace(
                go.Bar(
                    x=scores[::-1],  # c-TFIDF ì ìˆ˜(ì—­ìˆœ: ë†’ì€ ì ìˆ˜ê°€ ìœ„ìª½ì—)
                    y=terms[::-1],  # í‚¤ì›Œë“œ(ì—­ìˆœ)
                    orientation='h',
                    marker_color=colors.get(t),
                    hovertemplate="%{y}<br>Score: %{x:.3f}<extra></extra>"
                ),
                row=row, col=col
            )

            # xì¶• ë ˆì´ë¸”(í•œê¸€)ê³¼ í°íŠ¸ ì§€ì •
            fig.update_xaxes(
                row=row, col=col,
                title_text="c-TF-IDF ",
                title_font_family=default_font,
                title_font_size=12,
                tickfont_family=default_font,
                tickfont_size=10
            )
            # yì¶• ë ˆì´ë¸”ì€ í‚¤ì›Œë“œ ìì²´ê°€ í‘œì‹œë˜ë¯€ë¡œ yì¶• ì œëª©ì€ ìƒëµ
            fig.update_yaxes(
                row=row, col=col,
                tickfont_family=default_font,
                tickfont_size=10
            )

        # â”€â”€â”€ 7. ì „ì²´ ë ˆì´ì•„ì›ƒ ìˆ˜ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        fig.update_layout(
            height=900,  # ë‘ í–‰ì´ë¯€ë¡œ ì ì ˆíˆ ë†’ì´ ì§€ì •
            width=1200,  # ì„¸ ì—´ì´ë¯€ë¡œ ë„ˆë¹„ í™•ë³´
            title_text="Topic 0~5: Top 12 words ë¶„í¬",
            title_font_family=default_font,
            title_font_size=24,
            showlegend=False,
            margin=dict(l=80, r=40, t=120, b=80)
        )

        # â”€â”€â”€ 8. ì„œë¸Œí”Œë¡¯ ì œëª©(Annotation) í°íŠ¸ ì§€ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for i in range(6):
            fig.layout.annotations[i].update(
                font=dict(family=default_font, size=14)
            )

        # Vectorizer ëª¨ë¸ ì„¤ì •
        vectorizer_model = CountVectorizer(vocabulary=list(vocab), ngram_range=(1, 1), min_df=2, token_pattern=r"(?u)\b\w[\w_]+\b")  # min_df ,max_df=0.5,

        # UMAP ëª¨ë¸ ì„¤ì •
        umap_model = umap.UMAP(n_neighbors=20, n_components=10, min_dist=0.01, metric='cosine', random_state=seed) # , spread=3.0)

        # HDBSCAN ëª¨ë¸ ì„¤ì •
        hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=40, min_samples=2, metric='euclidean', cluster_selection_method='eom', prediction_data=True) #min_samples=2,

        # cTF-IDF ëª¨ë¸ ì„¤ì •
        ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=False)   #bm25_weighting=True, reduce_frequent_words=True)

        from sentence_transformers import SentenceTransformer, util
        # 2) í•œêµ­ì–´ íŠ¹í™” SBERT ë¡œë“œ
        embedding_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
        #def grid():
        # í™˜ê²½ ë³€ìˆ˜ ë° ì‹œë“œ ì„¤ì •
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        seed = 42
        #print(f'len:{len(lemmatized_patents)}')
        # ë¬¸ì„œ í† í°í™” ì¤€ë¹„
        texts = [doc.split() for doc in lemmatized_patents]
        dictionary = Dictionary(texts)

        #í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
        # param_grid = {
        #     "n_neighbors": [10, 15, 20, 30, 40, 50],
        #     "n_components": [5, 8, 10],
        #     "min_dist": [0, 0.01],
        #     "min_cluster_size": [15, 20, 25, 30, 40, 50]
        # }

        param_grid = {
            "n_neighbors": [10],
            "n_components": [5],
            "min_dist": [0],
            "min_cluster_size": [15]
        }


        # ì´ ì¡°í•© ìˆ˜ ê³„ì‚°
        total_combinations = (
            len(param_grid["n_neighbors"]) *
            len(param_grid["n_components"]) *
            len(param_grid["min_dist"]) *
            len(param_grid["min_cluster_size"])
        )

        results = []

        # ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        output_path = "../0601_grid_search_results1154.json"

        # (1) ë¹ˆ JSON íŒŒì¼ì„ ë¯¸ë¦¬ ìƒì„±í•´ ë‘ë©´, ì´í›„ ë®ì–´ì“°ê¸°ê°€ í¸í•´ì§‘ë‹ˆë‹¤.
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump({"results": [], "best": None}, f, ensure_ascii=False, indent=4)

        # Grid search with progress bar
        for grid_idx, (n_neighbors, n_components, min_dist, min_cs) in enumerate(tqdm(
                itertools.product(
                    param_grid["n_neighbors"],
                    param_grid["n_components"],
                    param_grid["min_dist"],
                    param_grid["min_cluster_size"]
                ),
                total=total_combinations,
                desc="Grid Search"
        )):
            # grid ì§„í–‰ìƒí™© ì €ì¥
            progress = {
                "stage": "grid",
                "current": grid_idx + 1,
                "total": total_combinations,
                "message": f"Grid {grid_idx + 1}/{total_combinations} ì¡°í•© í‰ê°€ ì¤‘"
            }
            with open("progress.json", "w", encoding="utf-8") as f:
                json.dump(progress, f, ensure_ascii=False)

            # 2.1) UMAP ëª¨ë¸ ì •ì˜
            umap_model = UMAP(
                n_neighbors=n_neighbors,
                n_components=n_components,
                min_dist=min_dist,
                metric='cosine',
                random_state=seed
            )
            # 2.2) HDBSCAN ëª¨ë¸ ì •ì˜
            hdbscan_model = HDBSCAN(
                min_cluster_size=min_cs,
                min_samples=2,
                metric='euclidean',
                cluster_selection_method='eom',
                prediction_data=True
            )

            # 2.3) BERTopic í•™ìŠµ
            topic_model = BERTopic(
                language="korean",
                calculate_probabilities=True,
                nr_topics='auto',
                top_n_words=15,
                vectorizer_model=vectorizer_model,
                embedding_model=embedding_model,
                umap_model=umap_model,
                hdbscan_model=hdbscan_model,
                ctfidf_model=ctfidf_model,
                verbose=False
            )
            try:
                topics, probs = topic_model.fit_transform(lemmatized_patents)
            except Exception as e:
                print("âŒ fit_transform ì¤‘ ì—ëŸ¬ ë°œìƒ:", e)
                return [], []  # í˜¹ì€ raise e ë¡œ ë‹¤ì‹œ ë˜ì ¸ë„ ë¨
            #print('-------------test1-------------')

            topic_words = {
                str(topic_id): [word for word, _ in topic_model.get_topic(topic_id)]
                for topic_id in topic_model.get_topic_freq().Topic
                if topic_id != -1
            }
            if len(topic_words) < 3:
                continue

            # -------------------------------------------------------------
            # 4) Coherence ê³„ì‚°
            # -------------------------------------------------------------
            coherence_per_topic = {}
            for topic_id, words in topic_words.items():
                cm = CoherenceModel(
                    topics=[words],
                    texts=texts,
                    dictionary=dictionary,
                    coherence='c_v'
                )
                coherence_per_topic[topic_id] = cm.get_coherence()
            mean_coh = sum(coherence_per_topic.values()) / len(coherence_per_topic)
            if mean_coh < 0.4:
                continue
            #print('-------------test2-------------')
            # -------------------------------------------------------------
            # 5) í† í”½ ë¹ˆë„(Count) ì €ì¥ (ë…¸ì´ì¦ˆ í¬í•¨)
            # -------------------------------------------------------------
            topic_freq_df = topic_model.get_topic_freq()
            topic_counts = {
                str(int(row["Topic"])): int(row["Count"])
                for _, row in topic_freq_df.iterrows()
            }

            # -------------------------------------------------------------
            # 6) í•´ë‹¹ ì¡°í•© ê²°ê³¼ ìƒì„±
            # -------------------------------------------------------------
            current_result = {
                "n_neighbors": n_neighbors,
                "n_components": n_components,
                "min_dist": min_dist,
                "min_cluster_size": min_cs,
                "coherence_per_topic": coherence_per_topic,
                "mean_coherence": mean_coh,
                "topic_counts": topic_counts,
                "topic_words": topic_words
            }
            results.append(current_result)
            #print('-------------test3-------------')

            # -------------------------------------------------------------
            # 7) í˜„ì¬ê¹Œì§€ì˜ resultsì™€ bestë¥¼ JSON íŒŒì¼ì— ì‹¤ì‹œê°„ ë®ì–´ì“°ê¸°
            # -------------------------------------------------------------
            # ìµœì  ì¡°í•©ì„ ê³„ì‚°í•˜ë ¤ë©´ ì§€ê¸ˆê¹Œì§€ì˜ results ì¤‘ ê°€ì¥ ë†’ì€ mean_coherenceë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            best_so_far = max(results, key=lambda x: x["mean_coherence"])
            output_dict = {"results": results, "best": best_so_far}

            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(output_dict, f, ensure_ascii=False, indent=4)

        best = max(results, key=lambda x: x["mean_coherence"]) if results else None
        if best:
            print(
                f"Grid search ì™„ë£Œ. ìµœì  ì¡°í•©: "
                f"{best['n_neighbors']}, {best['n_components']}, "
                f"{best['min_dist']}, {best['min_cluster_size']} "
                f"(í‰ê·  Coherence: {best['mean_coherence']:.4f})"
            )
        else:
            print("ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")


        # BERTopic ëª¨ë¸ ì´ˆê¸°í™” ë° í›ˆë ¨
        topic_model = BERTopic(
            language="korean",  # ì–¸ì–´ ì„¤ì •
            calculate_probabilities=True,  # í™•ë¥  ê³„ì‚° ì—¬ë¶€
            nr_topics='auto',  # ì£¼ì œì˜ ìˆ˜ ì œí•œ, autoë¡œ í•´ì•¼ HDBSCAN ì‘ë™
            top_n_words=15,  # ê° ì£¼ì œì˜ ìƒìœ„ ë‹¨ì–´ ìˆ˜
            # ì£¼ì œì˜ ìµœì†Œ í¬ê¸°
            vectorizer_model=vectorizer_model,  # ë²¡í„°í™” ëª¨ë¸
            embedding_model=embedding_model,  # ì„ë² ë”© ëª¨ë¸
            umap_model=umap_model,  # UMAP ëª¨ë¸
            hdbscan_model=hdbscan_model,  # HDBSCAN ëª¨ë¸
            ctfidf_model=ctfidf_model,  # c-TFIDF ëª¨ë¸
            #representation_model=representation_model,
            verbose=True  # ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€
        )

        # ì£¼ì œ ëª¨ë¸ í›ˆë ¨
        #print(f'check:{lemmatized_patents}')
        topics, probabilities = topic_model.fit_transform(lemmatized_patents)
        
        #-----------------GTM1-----------------
        #------------------------------------------------------------------
        
        # í† í”½ ë²ˆí˜¸ë¥¼ ê°€ì ¸ì˜¤ê³  ì²« ë²ˆì§¸ í† í”½(-1)ì„ ì œì™¸
        topics_dict = {}
        for topic_num in list(topic_model.get_topics())[1:]:  # -1 í† í”½(ë…¸ì´ì¦ˆ) ì œì™¸
            words = [word for word, _ in topic_model.get_topic(topic_num)]
            topics_dict[topic_num] = words

        return topics_dict
#------------------------------------------------------------------------                       
    